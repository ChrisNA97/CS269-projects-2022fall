---
layout: post
comments: true
title: Off-policy Meta Reinforcement Learning in a Multi-Agent Competitive Environment
author: Christian Natajaya (Team 21)
date: 2022-12-8
---


## Abstract
 
In this project, we will investigate different applications of meta-RL. Previous investigations have been centered on using meta-RL to expedite single agents learning new unseen tasks drawn from the same task distribution. In this project, we will explore how meta-RL trained agents perform in a competitive multi-agent RL setting when having to play against new unseen agents. We will specifically implement the PEARL (off-policy meta-RL) algorithm developed by Kate Rakelly, et al. Instead of relying on LSTM recurrence in the policy network to memorize skills, PEARL decouples this from the policy by using a network to encode previous experiences into a latent context variable. This latent context variable is used to condition both actor (policy) and critic (value) architectures, We will use Derkgym as the multi-agent gym environment to test the PEARL algorithm. We will first train the agent against another agent that will only perform random actions. After collecting this experience, we will then expose the agent to new opponents with different skillsets. Using meta-RL, we anticipate that the agent will be able to adapt to new opponents quicker.


## Introduction

Conventional reinforcement learning algorithms aim to learn a policy that maximizes the trajectory rewards for a given task (**eq.1**). This learned policy generally does not translate over to new unseen tasks; a distribution shift in the observation space may yield the wrong actions, and a change in the environment rewards and transitions means the same actions may yield suboptimal trajectories. Therefore, conventional reinforcement learning algorithms need to learn a separate policy for each new task, makig it extremely sample inefficient. In contrast, meta reinforcement learning algorithms are able to learn new and unseen tasks more efficiently, much like how people can learn new skills quickly using previously learned skills.

In meta reinforcement learning, the algorithm models a function that takes in a Markov decision process as input $f_{\theta}$ (**eq.3**). It then learns to optimize this function so that the policy maximizes the trajectory reward over any Markov decision process that is drawn from the task distribution. This can be accomplished by learning how to encode context from experience and using this context to condition a policy that universally applies to all tasks in the distribution. PEARL - an efficient off-policy meta reinforcement learning algorithm - operates on this basis as explained later under the **Algorithm** section.

**Reinforcement Learning Objective**

$$ eq.1 \;\; \theta^* = argmax_{\theta} E_{\pi_{\theta}(\tau)}\[R(\tau)\] \\ $$ 

**Meta Reinforcement Learning Objective**

$$ eq.2 \;\; \theta^* = argmax_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}\[R(\tau)\] \\ $$
 
$$ \phi_{i} = f_{\theta}(M_{i}) \\ $$

This project will apply the PEARL algorithm to Derk's gym environment. In meta reinforcement learning, the train and test tasks are generally different from each other but are drawn from the same distribution. We will achieve this problem by creating train and test tasks that consist of agents with a random combination of abilities, which effectively changes the agents' observations and possible actions in every task. We suspect that conventional reinforcement learning algorithms will struggle to learn a policy that is able to adapt and play against agents with randomized abilities it has not seen before (or has not seen frequently enough).

With meta reinforcement learning, we hope that by exposing the algorithm to a variety of agents with different abilities it is able to infer context and use this to optimize a policy to solve the unseen task. Essentially, the PEARL algorithm should be able to use its experience playing against a random assortment of agents as context in order to to create an action hypotheses for unseen tasks. The algorithm will then evaluate this hypothesis and learn to make better context inferences and policy outputs.


## Algorithm

> BASELINE

The baseline algorithm (used to represent conventional reinforcement learning) models the policy as a neural network that takes in the observation as input and outputs the actions. We apply evolutionary strategy to train the neural network; at every iteration, we perturb the neural network weights with a Gaussian noise and pick the weights that lead to the highest path reward.

> PEARL

<p align="center">
    <img width="300" src="https://github.com/ChrisNA97/CS269-projects-2022fall/blob/main/assets/images/team21/MetaRL%20Loop.png" alt="PEARL Training Loop">
<p align="center">
 Fig 1. PEARL Algorithm: The inference network qÏ† uses context data to infer the posterior over the latent context variable Z, which conditions the actor and critic, and is optimized with gradients from the critic as well as from an information bottleneck on Z. 
</p>

**Inference network (context encoder) and latent context variable**

We sample our experiences in the replay buffer for context, which we use to hypothesize what actions to perform for the current unseen task. This is done by passing the context (C) into an inference network $q(z\|c)$ to generate a latent probabilistic variable (Z), which we then sample and use to condition the policy given by $\pi_{\theta}(a|s, z)$. Effectively, the context is used to adapt the agents' policy behaviors to the new unseen task. The weights of the inference network are optimized jointly with the weights of the actor network $\pi_{\theta}(a \| s,z)$ and critic network $Q_{\theta}(s,a,z), using gradients from the critic update - which will be discussed further below.

**Soft actor critic reinforcement learning**

The PEARL algorithm is built on top of the soft actor-critic algorithm (SAC), which is an off-policy method that augments the traditional sum of discounted returns with the policy entropy. It consists of two q-value functions (select the minimum), one value function (target and actual), and one policy function. We optimize the parameters of the value network using the mean-squared error loss between... and the ADAM optimizer. We optimize the parameters of the q-value networks using the mean-squared error loss between... and the ADAM optimizer. We optimize the paramaters of the policy network using...

> Update Rules

**Loss Functions**

$$ L_{V} = E_{s,a,r,s'\sim B, z\sim q_{\phi}(z|c)} \[V_{\phi}(s,z) - min_{i=1,2}Q_{\theta, i}(s',a',z) - log \pi_{\theta}(a'\|s')\] \\ $$

$$ L_{Q} = E_{s,a,r,s'\sim B, z\sim q_{\phi}(z|c)} \[Q_{\theta}(s,a,z) - (r+V_{\phi}(s',z))\]^2 \\ $$

$$ L_{\pi} = E_{s \sim B, a \sim \pi_{\theta}, z \sim q_{\phi}(z|c)} \left[D_{KL}\left(\pi_{\theta}(a|s,z) \|\| \frac{\exp{Q_{\theta}(s,a,z)}}{Z_\theta (s)}\right)\right] \\ $$

**Weight Stochastic Gradient Descent**

$$ \phi_{V} \leftarrow \phi - \alpha \nabla_{\phi} \sum_{i} (L_{V}^{i}) \\ $$

$$ \theta_{Q} \leftarrow \theta_{Q} - \alpha \nabla_{\theta_{Q}} \sum_{i} L_{Q}^{i} \\ $$

$$ \theta_{\pi} \leftarrow \theta_{\pi} - \alpha \nabla_{\theta_{\pi}} \sum_{i} L_{\pi}^{i} \\ $$

$$ \phi_{q(z \| c} \leftarrow \phi - \alpha \nabla_{\phi} \sum_{i} (L_{Q}^{i} + L_{KL}^{i}) \\ $$

## Methodology

> Meta Training Algorithm

Pseudocode

> Derk's Gym Environment

Changed the environment from MuJoCo to Derk's Gym. Multi-agent reinforcement learning with one policy controlling the actions of 3 different agents. 

<p align="center">
    <img width="300" src="https://github.com/ChrisNA97/CS269-projects-2022fall/blob/main/assets/images/team21/Derk%20Gym.png" alt="Derk's Gym Environment">
<p align="center">
 Fig 2. Derk's Gym Environment. 
</p>

## Results

The agents learned to focus on the enemy tower and the enemies. However, the agent hesitates to attack since they have different abilities and are unable to learn how to use them.

> Average path reward while collecting context data

| Trajectory | Baseline | PEARL |
| ---------- | -------- | ----- |
|    1       |          |       |
|    2       |          |       |
|    3       |          |       |
|    4       |          |       |
|    5       |          |       |
|    6       |          |       |
|    7       |          |       |
|    8       |          |       |
|    9       |          |       |
|    10      |          |       |

> Future Improvements

Several improvements could be made to the environment. Firstly, the environment does not allow for negative rewards. As a result, agents are not penalized for killing a teammate or destroying its own tower and the policy continues returning such actions. Secondly, the environment returns a positive reward when an opposing agent dies or an opposing tower is destroyed, regardless of who is responsible for the action (even if the opposing agent is responsible for the action). This causes the algorithm to learn actions that do not necessarily lead to the rewards. Thidly, the maximum step length could be increased since several episodes terminate before the agent has successfully killed an opposing agent or destroyed an opposing tower. This leads to some episodes with zero rewards even if the agent has started taking favorable actions.


## Reference
Derkgym documentation: http://docs.gym.derkgame.com/#examples
Random agent in Derkgym: https://github.com/MountRouke/Randos
PEARL code: https://github.com/katerakelly/oyster

[1] Dwibedi, Debidatta, et al. "Counting out time: Class agnostic video repetition counting in the wild." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.   

[Peng, et al.] Peng, Zhenghao, et al. "Maybe you can also use other format for reference as you wish." Nature. 2022. 

---
