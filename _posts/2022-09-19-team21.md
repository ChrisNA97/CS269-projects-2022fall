---
layout: post
comments: true
title: Off-policy Meta Reinforcement Learning in a Multi-Agent Competitive Environment
author: Christian Natajaya (Team 21)
date: 2022-12-8
---


## Abstract
 
In this project, we will investigate different applications of meta-RL. Previous investigations have been centered on using meta-RL to expedite single agents learning new unseen tasks drawn from the same task distribution. In this project, we will explore how meta-RL trained agents perform in a competitive multi-agent RL setting when having to play against new unseen agents. We will specifically implement the PEARL (off-policy meta-RL) algorithm developed by Kate Rakelly, et al. Instead of relying on LSTM recurrence in the policy network to memorize skills, PEARL decouples this from the policy by using a network to encode previous experiences into a latent context variable. This latent context variable is used to condition both actor (policy) and critic (value) architectures, We will use Derkgym as the multi-agent gym environment to test the PEARL algorithm. We will first train the agent against another agent that will only perform random actions. After collecting this experience, we will then expose the agent to new opponents with different skillsets. Using meta-RL, we anticipate that the agent will be able to adapt to new opponents quicker.


## Introduction

Conventional reinforcement learning algorithms learn a separate policy for each new task. This is extremely sample inefficient and does not trend towards the direction of general artificial intelligence. We want a reinforcement learning algorithm that is able to learn new tasks more efficiently, much like how people can learn new skills quickly because we never learn from scratch.

Usually the train and test tasks are different but drawn from the same family of problems. In this project, our training task will consist of a random assortment of agents... In every task, the three agents being controlled will have a different combination of abilities...

**Reinforcement Learning Objective**

$$ \theta^* = argmax_{\theta} E_{\pi_{\theta}(\tau)}\[R(\tau)\] \\ $$ 

**Meta Reinforcement Learning Objective**

$$ \theta^* = argmax_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}\[R(\tau)\] \\ $$
 
$$ \phi_{i} = f_{\theta}(M_{i}) \\ $$

Training a meta reinforcement learning algorithm consists of leveraging data from a variety of training tasks to learn to infer the value of Z from a recent history of experience in the new task, as well as optimizing the policy to solve the task given samples from the posterior over Z. Essentially use context to create hypotheses for unseen tasks and then evaluate whether it’s correct.

## Algorithm

> BASELINE

We use a simple multi-layer perceptron that takes in the observations and outputs the actions. We perturb the weight with random noise at every iteration and evaluate the actions. We keep the weights that retain the higher rewards and continue moving in that direction by updating the distribution mean.

> PEARL

**Inference network (context encoder) and latent context variable**

We sample our experiences in the replay buffer for context, which we use to hypothesize how the current unseen task should be performed. This is done by passing the context (C) into an inference network $q(z\|c)$ to generate a latent probabilistic variable (Z), which we then use to condition the policy as $\pi_{\theta}(a|s, z)$. Effectively, the context is used to adapt the agents' behaviors to the new unseen task. 

We optimize the parameters of the inference network jointly with the parameters of the actor πθ(a|s, z) and critic Qθ(s, a, z), using the reparameterization trick to compute gradients for parameters of qφ(z|c) through sampled z’s. We train the inference network using gradients from the Bellman update for the critic. 

**Soft actor critic reinforcement learning**

The PEARL algorithm is built on top of the soft actor-critic algorithm (SAC), which is an off-policy method that augments the traditional sum of discounted returns with the policy entropy. It consists of two q-value functions (select the minimum), one value function (target and actual), and one policy function. 

We optimize the parameters of the value network using the mean-squared error loss between... and the ADAM optimizer. We optimize the parameters of the q-value networks using the mean-squared error loss between... and the ADAM optimizer. We optimize the paramaters of the policy network using...

> Update Rules

**Loss Functions**

$$ L_{V} = E_{s,a,r,s'\sim B, z\sim q_{\phi}(z|c)} \[V_{\phi}(s,z) - min_{i=1,2}Q_{\theta, i}(s',a',z) - log \pi_{\theta}(a'\|s')\] \\ $$

$$ L_{Q} = E_{s,a,r,s'\sim B, z\sim q_{\phi}(z|c)} \[Q_{\theta}(s,a,z) - (r+V_{\phi}(s',z))\]^2 \\ $$

$$ L_{\pi} = E_{s \sim B, a \sim \pi_{\theta}, z \sim q_{\phi}(z|c)} \left[D_{KL}\left(\pi_{\theta}(a|s,z) \|\| \frac{\exp{Q_{\theta}(s,a,z)}}{Z_\theta (s)}\right)\right] \\ $$

**Weight Stochastic Gradient Descent**

$$ \phi_{V} \leftarrow \phi - \alpha \nabla_{\phi} \sum_{i} (L_{V}^{i}) \\ $$

$$ \theta_{Q} \leftarrow \theta_{Q} - \alpha \nabla_{\theta_{Q}} \sum_{i} L_{Q}^{i} \\ $$

$$ \theta_{\pi} \leftarrow \theta_{\pi} - \alpha \nabla_{\theta_{\pi}} \sum_{i} L_{\pi}^{i} \\ $$

$$ \phi_{q(z \| c} \leftarrow \phi - \alpha \nabla_{\phi} \sum_{i} (L_{Q}^{i} + L_{KL}^{i}) \\ $$

## Methodology

> Derk's Gym Environment

Changed the environment from MuJoCo to Derk's Gym. Multi-agent reinforcement learning with one policy controlling the actions of 3 different agents. 

> Meta Training Loop

The inference network qφ uses context data to infer the posterior over the latent context variable Z, which conditions the actor and critic, and is optimized with gradients from the critic as well as from an information bottleneck on Z. De-coupling the data sampling strategies for context (SC) and RL batches is important for off-policy learning.

![Alt text](https://github.com/ChrisNA97/CS269-projects-2022fall/blob/main/assets/images/team21/MetaRL%20Loop.png =50x100)

*Fig 1. PEARL Training Loop*.

## Results

The agents learned to focus on the enemy tower and the enemies. However, the agent hesitates to attack since they have different abilities and are unable to learn how to use them.

> Average path reward while collecting context data**

> Future Improvements

Several improvements could be made to the environment. Firstly, the environment does not allow for negative rewards. As a result, agents are not penalized for killing a teammate or destroying its own tower and the policy continues returning such actions. Secondly, the environment returns a positive reward when an opposing agent dies or an opposing tower is destroyed, regardless of who is responsible for the action (even if the opposing agent is responsible for the action). This causes the algorithm to learn actions that do not necessarily lead to the rewards. Thidly, the maximum step length could be increased since several episodes terminate before the agent has successfully killed an opposing agent or destroyed an opposing tower. This leads to some episodes with zero rewards even if the agent has started taking favorable actions.


## Reference
Derkgym documentation: http://docs.gym.derkgame.com/#examples
Random agent in Derkgym: https://github.com/MountRouke/Randos
PEARL code: https://github.com/katerakelly/oyster

[1] Dwibedi, Debidatta, et al. "Counting out time: Class agnostic video repetition counting in the wild." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.   

[Peng, et al.] Peng, Zhenghao, et al. "Maybe you can also use other format for reference as you wish." Nature. 2022. 

---
