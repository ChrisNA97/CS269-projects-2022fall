---
layout: post
comments: true
title: Post Template
author: Christian Natajaya (Team 21)
date: 2022-10-15
---


## Abstract
 
In this project, we will investigate different applications of meta-RL. Previous investigations have been centered on using meta-RL to expedite single agents learning new unseen tasks drawn from the same task distribution. In this project, we will explore how meta-RL trained agents perform in a competitive multi-agent RL setting when having to play against new unseen agents. We will specifically implement the PEARL (off-policy meta-RL) algorithm developed by Kate Rakelly, et al. Instead of relying on LSTM recurrence in the policy network to memorize skills, PEARL decouples this from the policy by using a network to encode previous experiences into a latent context variable. This latent context variable is used to condition both actor (policy) and critic (value) architectures, We will use Derkgym as the multi-agent gym environment to test the PEARL algorithm. We will first train the agent against another agent that will only perform random actions. After collecting this experience, we will then expose the agent to new opponents with different skillsets. Using meta-RL, we anticipate that the agent will be able to adapt to new opponents quicker.


## Introduction

Conventional reinforcement learning algorithms learn a separate policy for each new task. This is extremely sample inefficient and does not trend towards the direction of general artificial intelligence. We want a reinforcement learning algorithm that is able to learn new tasks more efficiently, much like how people can learn new skills quickly because we never learn from scratch.

Usually the train and test tasks are different but drawn from the same family of problems. In this project, our training task will consist of a random assortment of agents... In every task, the three agents being controlled will have a different combination of abilities...

$$ \theta^* = argmax_{\theta} E_{\pi_{\theta}(\tau)}\[R(\tau)\] $$ \\
$$ \theta^* = argmax_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}\[R(\tau)\] $$ \\
$$ \phi_{i} = f_{\theta}(M_{i}) $$

## Algorithm

> BASELINE

We use a simple multi-layer perceptron that takes in the observations and outputs the actions. We perturb the weight with random noise at every iteration and evaluate the actions. We keep the weights that retain the higher rewards and continue moving in that direction by updating the distribution mean.

> PEARL

**Inference network (context encoder) and latent context variable**
We capture knowledge about how the current task should be performed in a latent probabilistic context variable Z (produced by the inference network), on which we condition the policy as πθ(a|s, z) in order to adapt its behavior to the task. Meta-training consists of leveraging data from a variety of training tasks to learn to infer the value of Z from a recent history of experience in the new task, as well as optimizing the policy to solve the task given samples from the posterior over Z.

**Soft actor critic reinforcement learning**
We build our algorithm on top of the soft actor-critic algorithm (SAC) (Haarnoja et al., 2018), an off-policy actorcritic method which augments the traditional sum of discounted returns with the entropy of the policy. We optimize the parameters of the inference network q(z|c) jointly with the parameters of the actor πθ(a|s, z) and critic Qθ(s, a, z), using the reparameterization trick (Kingma & Welling, 2014) to compute gradients for parameters of qφ(z|c) through sampled z’s. We train the inference network using gradients from the Bellman update for the critic. We found empirically that training the encoder to recover the state-action value function outperforms optimizing it to maximize actor returns, or reconstruct states and rewards.

$$ L_{critic} = E_{s,a,r,s'\tildeB \\ z\tildeq_{\phi}(z|c)$$ \\
$$ L_{actor} = $$

## Methodology

Changed the environment... Multi-agent reinforcement learning with one policy controlling the actions of 3 different agents. 

## Results

The agents learned to focus on the enemy tower and the enemies. However, the agent hesitates to attack since they have different abilities and are unable to learn how to use them.

> Average path reward while collecting context data**




## Basic Syntax
### Image
Please create a folder with the name of your team id under `/assets/images/`, put all your images into the folder and reference the images in your main content.

You can add an image to your survey like this:
![YOLO]({{ '/assets/images/team00/object_detection.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. YOLO: An object detection method in computer vision* [1].

Please cite the image if it is taken from other people's work.


### Table
Here is an example for creating tables, including alignment syntax.

|             | column 1    |  column 2     |
| :---        |    :----:   |          ---: |
| row1        | Text        | Text          |
| row2        | Text        | Text          |



### Code Block
```
# This is a sample code block
import torch
print (torch.__version__)
```

## Reference
Derkgym documentation: http://docs.gym.derkgame.com/#examples
Random agent in Derkgym: https://github.com/MountRouke/Randos
PEARL code: https://github.com/katerakelly/oyster

[1] Dwibedi, Debidatta, et al. "Counting out time: Class agnostic video repetition counting in the wild." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.   

[Peng, et al.] Peng, Zhenghao, et al. "Maybe you can also use other format for reference as you wish." Nature. 2022. 

---
