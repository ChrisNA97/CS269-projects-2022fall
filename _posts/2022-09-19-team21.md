---
layout: post
comments: true
title: Off-policy Meta Reinforcement Learning in a Multi-Agent Competitive Environment
author: Christian Natajaya (Team 21)
date: 2022-12-8
---


## Abstract
 
In this project, we will investigate different applications of meta-RL. Previous investigations have been centered on using meta-RL to expedite single agents learning new unseen tasks drawn from the same task distribution. In this project, we will explore how meta-RL trained agents perform in a competitive multi-agent RL setting when having to play against new unseen agents. We will specifically implement the PEARL (off-policy meta-RL) algorithm developed by Kate Rakelly, et al. Instead of relying on LSTM recurrence in the policy network to memorize skills, PEARL decouples this from the policy by using a network to encode previous experiences into a latent context variable. This latent context variable is used to condition both actor (policy) and critic (value) architectures, We will use Derkgym as the multi-agent gym environment to test the PEARL algorithm. We will first train the agent against another agent that will only perform random actions. After collecting this experience, we will then expose the agent to new opponents with different skillsets. Using meta-RL, we anticipate that the agent will be able to adapt to new opponents quicker.


## Introduction

Conventional reinforcement learning algorithms aim to learn a policy that maximizes the trajectory rewards for a given task or Markov decision process (**eq.1**). This learned policy generally does not translate over to new unseen tasks; a distribution shift in the observation space may cause the policy to perform suboptimal actions, and a change in the environment rewards and state transitions means the same actions may yield different trajectories. Therefore, conventional reinforcement learning algorithms need to learn a separate policy for each new task, makig it extremely sample inefficient. 

In contrast, meta reinforcement learning algorithms are able to learn new and unseen tasks more efficiently, much like how people can learn new skills quickly using previously learned skills. In meta reinforcement learning, the algorithm models a function that takes in a Markov decision process as input $f_{\theta}$ (**eq.3**). It then learns to optimize this function so that the policy maximizes the trajectory reward over any Markov decision process that is drawn from the task distribution. This can be accomplished by learning how to encode context from experience and using this encoding to condition a policy so that it universally applies to every task in the distribution. PEARL - an efficient off-policy meta reinforcement learning algorithm - operates on this basis as explained later in **Algorithm**.

**Reinforcement Learning Objective**

$$ eq.1 \;\; \theta^* = argmax_{\theta} E_{\pi_{\theta}(\tau)}\[R(\tau)\] \\ $$ 

**Meta Reinforcement Learning Objective**

$$ eq.2 \;\; \theta^* = argmax_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}\[R(\tau)\] \\ $$
 
$$ \phi_{i} = f_{\theta}(M_{i}) \\ $$

This project will apply the PEARL algorithm to Derk's gym, a multi-agent competitive environment. In meta reinforcement learning, the train and test tasks are generally different from each other but are drawn from the same distribution. We will achieve this problem by creating train and test tasks that consist of agents with a random combination of abilities, which effectively changes the agents' observations and the action rewards in every task. We hypothesize that conventional reinforcement learning algorithms will struggle to learn a policy that is able to adapt and play against agents with randomized abilities it has not seen before, or has not seen frequently enough.

With meta reinforcement learning, we hope that by exposing the algorithm to a variety of agents with different abilities, it is able to learn to make inferences from context and use this to optimize a policy to solve the unseen task. In other words, the PEARL algorithm should be able to use its experience playing against an assortment of agents as context to create an action hypothesis when it plays against a combination of agents it has not seen before. The PEARL algorithm will then evaluate this hypothesis and learn to make better context inferences and policy outputs.


## Algorithm

> BASELINE

The baseline algorithm (used to represent conventional reinforcement learning) models the policy as a neural network that takes in the observation as input and outputs the actions. We apply evolutionary strategy to train the neural network; at every iteration, we perturb the neural network weights with a Gaussian noise and pick the weights that lead to the highest path reward.

> PEARL

<p align="center">
    <img width="300" src="https://github.com/ChrisNA97/CS269-projects-2022fall/blob/main/assets/images/team21/MetaRL%20Loop.png" alt="PEARL Training Loop">
<p align="center">
 Fig 1. PEARL Algorithm: The inference network qÏ† uses context data to infer the posterior over the latent context variable Z, which conditions the actor and critic, and is optimized with gradients from the critic as well as from an information bottleneck on Z. 
</p>

**Inference network (context encoder) and latent context variable**

This is the main novelty of the PEARL algorithm. The PEARL algorithm retrieves experiences in the replay buffer for context, which it uses to hypothesize what actions to perform for the current task. This is done by passing the context (C) into an inference network $q(z\|c)$ to generate a probabilistic latent variable Z, which is effectively a posterior over the context. This probabilistic latent variable is then sampled and used to condition the policy given by $\pi_{\theta}(a|s, z)$. Essentially, the context is used to adapt the agents' policy behaviors to the new unseen task. The weights of the inference network are optimized jointly with the weights of the actor-critic networks using gradients from the critic update - which will be discussed further below.

**Soft actor critic reinforcement learning**

The PEARL algorithm is built on top of the soft actor-critic algorithm (SAC), which is an off-policy method that augments the discounted returns with the policy entropy. The critic consists of two q-value networks $Q_{\theta}(s,a,z)$ and one value network $(V_{\phi}(s,z))$, and the actor consists of one policy function $\pi_{\theta}(a \| s,z)$. As mentioned earlier, the novelty of PEARL is that these actor-critic networks also take in the latent variable returned by the inference network as input, so that they are conditioned by the context posterior. We optimize the parameters of the actor-critic networks using the ADAM optimizer and the loss functions below in **Update Rules**.

> Update Rules

**Loss Functions**

$$ L_{V} = E_{s,a,r,s'\sim B, z\sim q_{\phi}(z|c)} \[V_{\phi}(s,z) - min_{i=1,2}Q_{\theta, i}(s',a',z) - log \pi_{\theta}(a'\|s')\] \\ $$

$$ L_{Q} = E_{s,a,r,s'\sim B, z\sim q_{\phi}(z|c)} \[Q_{\theta}(s,a,z) - (r+V_{\phi}(s',z))\]^2 \\ $$

$$ L_{\pi} = E_{s \sim B, a \sim \pi_{\theta}, z \sim q_{\phi}(z|c)} \left[D_{KL}\left(\pi_{\theta}(a|s,z) \|\| \frac{\exp{Q_{\theta}(s,a,z)}}{Z_\theta (s)}\right)\right] \\ $$

**Stochastic Gradient Descent**

$$ \phi_{V} \leftarrow \phi - \alpha \nabla_{\phi} \sum_{i} L_{V}^{i} \\ $$

$$ \theta_{Q} \leftarrow \theta_{Q} - \alpha \nabla_{\theta_{Q}} \sum_{i} L_{Q}^{i} \\ $$

$$ \theta_{\pi} \leftarrow \theta_{\pi} - \alpha \nabla_{\theta_{\pi}} \sum_{i} L_{\pi}^{i} \\ $$

$$ \phi_{q(z \| c} \leftarrow \phi - \alpha \nabla_{\phi} \sum_{i} (L_{Q}^{i} + L_{KL}^{i}) \\ $$


## Contribution

> Derk's Gym Environment

In this project, we changed the environment from MuJoCo -- a single agent environment -- to Derk's Gym -- a multi-agent evnrionment. In MuJoCo, the agent trained using the PEARL algorithm was able to successfully learn new tasks quicker than other RL methods. For example, a half cheetah trained to move in one velocity was able to learn to run in another target velocity during testing. We wish to explore whether the PEARL algorithm translates over to multi-agent environments. Specifically during training, we will let the agents have different abilities.. and play against other agents (which are controlled by a random policy) with different abilities. During testing, we will provide the algorithm with a different combination ofabilities it has not seen before. The inference network is able to encode strategies it learned through its experiences...

We use the PEARL algorithm written by Kate Rakelly, et al. as the basis of our code. The contribution of this project is to transfer over Multi-agent reinforcement learning with one policy controlling the actions of 3 different agents. For each agent, the observation space is 64 (some of the observations include agent having a certain ability, opposing agent having a certain ability, map position). For each agent, the action space is 5 (some of the actions include rotate, move, and use abilities). Therefore, we concatenated the observation space and action space of 3 agents when developing the inference network and actor and critic.

<p align="center">
    <img width="300" src="https://github.com/ChrisNA97/CS269-projects-2022fall/blob/main/assets/images/team21/Derk%20Gym.png" alt="Derk's Gym Environment">
<p align="center">
 Fig 2. Derk's Gym Environment. 
</p>

## Results

The agents trained using the baseline algorithm... The agents trained using PEARL learned to focus on the enemy tower and the enemies. However, the agent hesitates to attack since they have different abilities and are unable to learn how to use them.

> Average path reward 

The table below summarizes the average of trajectory rewards across all agents for a single game episode. This was retrieved during testing, when the policy was made to play with 3 agents with an unseen combination of abilities, against another 3 agents also with an unseen combination of abilities controlled by a random policy.

| Game | Baseline Reward | PEARL Reward | 
| ---- | --------------- | ------------ | 
|    1 |          100    |      0       |
|    2 |          -100   |     30       |
|    3 |          -100   |     106      |
|    4 |          -100   |      90      |
|    5 |           100   |      20      |
|    6 |           100   |     120      |
|    7 |          -100   |      18      |
|    8 |          -100   |      0       |
|    9 |           100   |      0       |
|    10|          -100   |      45      |
| Mean |          -10    |      43      |


> Future Improvements

Several improvements could be made to the environment. First, the environment returns a positive reward when an opposing agent dies or an opposing tower is destroyed, regardless of who is responsible for the action (even if the opposing agent is responsible for the action). This causes the algorithm to learn actions that do not necessarily lead to the rewards. Second, the environment does sufficient penalize agent actions that kill a teammate or destroy its own tower. The rewards could be made more negative so the policy is discouraged from performing such actions. Finally, the maximum step length of the algorithm could be increased since several episodes terminate before the agent has successfully killed an opposing agent or destroyed an opposing tower. This leads to some episodes with zero rewards even if the agent has started taking favorable actions, so the policy fails to learn these optimal behavior.


## Reference
[1] Derkgym documentation: http://docs.gym.derkgame.com/#examples
[2] Random agent in Derkgym: https://github.com/MountRouke/Randos
[3] PEARL code: https://github.com/katerakelly/oyster
[4] Meta reinforcement learning: https://metalearning-cvpr2019.github.io/assets/CVPR_2019_Metalearning_Tutorial_Chelsea_Finn.pdf
[5] PEARL paper: https://arxiv.org/pdf/1903.08254.pdf

[example] Dwibedi, Debidatta, et al. "Counting out time: Class agnostic video repetition counting in the wild." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.   

---
